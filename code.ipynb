{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['GeeksforGeeks', 'is', 'for', 'geeks']\n"]}],"source":["\n","# import WhitespaceTokenizer() method from nltk \n","from nltk.tokenize import WhitespaceTokenizer \n","     \n","# Create a reference variable for Class WhitespaceTokenizer \n","tk = WhitespaceTokenizer() \n","     \n","# Create a string input \n","gfg = \"GeeksforGeeks \\nis\\t for geeks\"\n","     \n","# Use tokenize method \n","geek = tk.tokenize(gfg) \n","     \n","print(geek) "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["##Dictionary based tokenisatoin\n","\n","from nltk import word_tokenize \n","from nltk.tokenize import MWETokenizer"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["General Word Tokenization \n"," ['Jammu', 'Kashmir', 'is', 'an', 'integral', 'part', 'of', 'India', '.', 'My', 'name', 'is', 'Pawan', 'Kumar', 'Gunjan', '.', 'He', 'is', 'from', 'Himachal', 'Pradesh', '.']\n","Dictionary based tokenization \n"," ['Jammu Kashmir', 'is', 'an', 'integral', 'part', 'of', 'India', '.', 'My', 'name', 'is', 'Pawan Kumar Gunjan', '.', 'He', 'is', 'from', 'Himachal Pradesh', '.']\n"]}],"source":["# import the necessary libraries \n","from nltk import word_tokenize \n","from nltk.tokenize import MWETokenizer \n","\n","# customn dictionary \n","dictionary = [(\"Jammu\", \"Kashmir\"), \n","\t\t\t(\"Pawan\", \"Kumar\", \"Gunjan\"), \n","\t\t\t(\"Himachal\", \"Pradesh\")] \n","\n","# Create an instance of MWETokenizer with the dictionary \n","Dictionary_tokenizer = MWETokenizer(dictionary, separator=' ') \n","\n","# Text \n","text = \"\"\" \n","Jammu Kashmir is an integral part of India. \n","My name is Pawan Kumar Gunjan. \n","He is from Himachal Pradesh. \n","\"\"\"\n","\n","tokens = word_tokenize(text) \n","print('General Word Tokenization \\n',tokens) \n","\n","dictionary_based_token =Dictionary_tokenizer.tokenize(tokens) \n","print('Dictionary based tokenization \\n',dictionary_based_token)\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n"]}],"source":["#Rule based\n","# Step 1: Load the input text \n","text = \"The quick brown fox jumps over the lazy dog.\"\n","\n","# Step 2: Define the tokenization rules (split on whitespace) \n","tokens = text.split() \n","\n","# Step 4: Output the tokens \n","print(tokens)\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Company Name: Geeks-for-Geeks\n","Email address: pawangunjan23@geeksforgeeks.com\n"]}],"source":["import re \n","\n","#Load the input text \n","text = \"Hello, I am working at Geeks-for-Geeks and my email is pawangunjan23@geeksforgeeks.com.\"\n","\n","#Define the regular expression pattern \n","p='([\\w]+-[\\w]+-[\\w]+)|([\\w\\.-]+@[\\w]+.[\\w]+)'\n","\n","# Find matches \n","matches = re.findall(p, text) \n","# print output \n","for match in matches: \n","\tif match[0]: \n","\t\tprint(f\"Company Name: {match[0]}\") \n","\telse: \n","\t\tprint(f\"Email address: {match[1]}\")\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["kites ---> kite\n","babies ---> baby\n","dogs ---> dog\n","flying ---> flying\n","smiling ---> smiling\n","driving ---> driving\n","died ---> died\n","tried ---> tried\n","feet ---> foot\n"]}],"source":["import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","\n","# Create WordNetLemmatizer object\n","wnl = WordNetLemmatizer()\n","\n","# single word lemmatization examples\n","list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling', \n","\t\t'driving', 'died', 'tried', 'feet']\n","for words in list1:\n","\tprint(words + \" ---> \" + wnl.lemmatize(words))\n","\t\n","#> kites ---> kite\n","#> babies ---> baby\n","#> dogs ---> dog\n","#> flying ---> flying\n","#> smiling ---> smiling\n","#> driving ---> driving\n","#> died ---> died\n","#> tried ---> tried\n","#> feet ---> foot\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on', 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n","the cat is sitting with the bat on the striped mat under many flying goose\n"]}],"source":["# sentence lemmatization examples\n","string = 'the cat is sitting with the bats on the striped mat under many flying geese'\n","\n","# Converting String into tokens\n","list2 = nltk.word_tokenize(string)\n","print(list2)\n","#> ['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on',\n","# 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n","\n","lemmatized_string = ' '.join([wnl.lemmatize(words) for words in list2])\n","\n","print(lemmatized_string) \n","#> the cat is sitting with the bat on the striped mat under many flying goose\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"name":"stdout","output_type":"stream","text":["[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'), ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'), ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('badly', 'RB'), ('flying', 'VBG'), ('geese', 'JJ')]\n","[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None), ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'), ('mat', 'n'), ('under', None), ('many', 'a'), ('badly', 'r'), ('flying', 'v'), ('geese', 'a')]\n","the cat be sit with the bat on the striped mat under many badly fly geese\n"]}],"source":["# WORDNET LEMMATIZER (with appropriate pos tags)\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import wordnet\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","# Define function to lemmatize each word with its POS tag\n","\n","# POS_TAGGER_FUNCTION : TYPE 1\n","def pos_tagger(nltk_tag):\n","\tif nltk_tag.startswith('J'):\n","\t\treturn wordnet.ADJ\n","\telif nltk_tag.startswith('V'):\n","\t\treturn wordnet.VERB\n","\telif nltk_tag.startswith('N'):\n","\t\treturn wordnet.NOUN\n","\telif nltk_tag.startswith('R'):\n","\t\treturn wordnet.ADV\n","\telse:\t\t \n","\t\treturn None\n","\n","sentence = 'the cat is sitting with the bats on the striped mat under many badly flying geese'\n","\n","# tokenize the sentence and find the POS tag for each token\n","pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence)) \n","\n","print(pos_tagged)\n","#>[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'), \n","# ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'), \n","# ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('flying', 'VBG'), ('geese', 'JJ')]\n","\n","# As you may have noticed, the above pos tags are a little confusing.\n","\n","# we use our own pos_tagger function to make things simpler to understand.\n","wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n","print(wordnet_tagged)\n","#>[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None), \n","# ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'), \n","# ('mat', 'n'), ('under', None), ('many', 'a'), ('flying', 'v'), ('geese', 'a')]\n","\n","lemmatized_sentence = []\n","for word, tag in wordnet_tagged:\n","\tif tag is None:\n","\t\t# if there is no available tag, append the token as is\n","\t\tlemmatized_sentence.append(word)\n","\telse:\t \n","\t\t# else use the tag to lemmatize the token\n","\t\tlemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n","lemmatized_sentence = \" \".join(lemmatized_sentence)\n","\n","print(lemmatized_sentence)\n","#> the cat can be sit with the bat on the striped mat under many fly geese\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[the, bats, saw, the, cats, with, best, stripes, hanging, upside, down, by, their, feet]\n","the bat see the cat with good stripe hang upside down by their foot\n"]}],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Create a Doc object\n","doc = nlp(u'the bats saw the cats with best stripes hanging upside down by their feet')\n","\n","# Create list of tokens from given string\n","tokens = []\n","for token in doc:\n","\ttokens.append(token)\n","\n","print(tokens)\n","#> [the, bats, saw, the, cats, with, best, stripes, hanging, upside, down, by, their, feet]\n","\n","lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n","\n","print(lemmatized_sentence)\n","#> the bat see the cat with good stripe hang upside down by -PRON- foot\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["program  :  program\n","programs  :  program\n","programmer  :  programm\n","programming  :  program\n","programmers  :  programm\n"]}],"source":["# import these modules\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","ps = PorterStemmer()\n","\n","# choose some words to be stemmed\n","words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n","\n","for w in words:\n","\tprint(w, \" : \", ps.stem(w))\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Programmers  :  programm\n","program  :  program\n","with  :  with\n","programming  :  program\n","languages  :  languag\n"]}],"source":["# importing modules\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","ps = PorterStemmer()\n","\n","sentence = \"Programmers program with programming languages\"\n","words = word_tokenize(sentence)\n","\n","for w in words:\n","\tprint(w, \" : \", ps.stem(w))\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" programm program with program languag\n"]}],"source":["from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from functools import reduce\n","\n","ps = PorterStemmer()\n","\n","sentence = \"Programmers program with programming languages\"\n","words = word_tokenize(sentence)\n","\n","# using reduce to apply stemmer to each word and join them back into a string\n","stemmed_sentence = reduce(lambda x, y: x + \" \" + ps.stem(y), words, \"\")\n","\n","print(stemmed_sentence)\n","#This code is contrinuted by Pushpa.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"DS","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":2}
